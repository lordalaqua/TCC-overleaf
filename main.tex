%
% exemplo genérico de uso da classe iiufrgs.cls
% $Id: iiufrgs.tex,v 1.1.1.1 2005/01/18 23:54:42 avila Exp $
%
% This is an example file and is hereby explicitly put in the
% public domain.
%
\def\crj#1{\textcolor{red}{(#1)}}
\documentclass[cic,tc]{iiufrgs}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetikzlibrary{calc,arrows.meta,positioning,backgrounds}
\usepackage{gensymb}
\tdplotsetmaincoords{-60}{-35}

% Para usar o modelo, deve-se informar o programa e o tipo de documento.
% Programas :
% * cic       -- Graduação em Ciência da Computação
% * ecp       -- Graduação em Ciência da Computação
% * ppgc      -- Programa de Pós Graduação em Computação
% * pgmigro   -- Programa de Pós Graduação em Microeletrônica
%
% Tipos de Documento:
% * tc                -- Trabalhos de Conclusão (apenas cic e ecp)
% * diss ou mestrado  -- Dissertações de Mestrado (ppgc e pgmicro)
% * tese ou doutorado -- Teses de Doutorado (ppgc e pgmicro)
% * ti                -- Trabalho Individual (ppgc e pgmicro)
%
% Outras Opções:
% * english    -- para textos em inglês
% * openright  -- Força início de capítulos em páginas ímpares (padrão da
% biblioteca)
% * oneside    -- Desliga frente-e-verso
% * nominatalocal -- Lê os dados da nominata do arquivo nominatalocal.def


% Use unicode
\usepackage[utf8]{inputenc}   % pacote para acentuação

% Necessário para incluir figuras
\usepackage{graphicx}         % pacote para importar figuras

\usepackage{times}            % pacote para usar fonte Adobe Times
% \usepackage{palatino}
% \usepackage{mathptmx}       % p/ usar fonte Adobe Times nas fórmulas

\usepackage[alf,abnt-emphasize=bf]{abntex2cite}	% pacote para usar citações abnt
\usepackage{todonotes}
%
% Informações gerais
%
\title{Estimativa de profundidade a partir de uma única imagem omnidirecional}

\author{Dal'Aqua}{Lorenzo Pezzi}

% orientador e co-orientador são opcionais (não diga isso pra eles :))
\advisor[Prof.~Dr.]{Jung}{Claudio}
% \coadvisor[Prof.~Dr.]{Knuth}{Donald Ervin}

% a data deve ser a da defesa; se nao especificada, são gerados
% mes e ano correntes
\date{11 de Janeiro}{2018}

% o local de realização do trabalho pode ser especificado (ex. para TCs)
% com o comando \location:
% \location{Itaquaquecetuba}{SP}

% itens individuais da nominata podem ser redefinidos com os comandos
% abaixo:
% \renewcommand{\nominataReit}{Prof\textsuperscript{a}.~Wrana Maria Panizzi}
% \renewcommand{\nominataReitname}{Reitora}
% \renewcommand{\nominataPRE}{Prof.~Jos{\'e} Carlos Ferraz Hennemann}
% \renewcommand{\nominataPREname}{Pr{\'o}-Reitor de Ensino}
% \renewcommand{\nominataPRAPG}{Prof\textsuperscript{a}.~Joc{\'e}lia Grazia}
% \renewcommand{\nominataPRAPGname}{Pr{\'o}-Reitora Adjunta de P{\'o}s-Gradua{\c{c}}{\~a}o}
% \renewcommand{\nominataDir}{Prof.~Philippe Olivier Alexandre Navaux}
% \renewcommand{\nominataDirname}{Diretor do Instituto de Inform{\'a}tica}
% \renewcommand{\nominataCoord}{Prof.~Carlos Alberto Heuser}
% \renewcommand{\nominataCoordname}{Coordenador do PPGC}
% \renewcommand{\nominataBibchefe}{Beatriz Regina Bastos Haro}
% \renewcommand{\nominataBibchefename}{Bibliotec{\'a}ria-chefe do Instituto de Inform{\'a}tica}
% \renewcommand{\nominataChefeINA}{Prof.~Jos{\'e} Valdeni de Lima}
% \renewcommand{\nominataChefeINAname}{Chefe do \deptINA}
% \renewcommand{\nominataChefeINT}{Prof.~Leila Ribeiro}
% \renewcommand{\nominataChefeINTname}{Chefe do \deptINT}

% A seguir são apresentados comandos específicos para alguns
% tipos de documentos.

% Trabalho Individual [ti]:
% \ti{123}     % numero do TI
% \ti[II]{456} % no caso de ser o segundo TI

%
% palavras-chave
% iniciar todas com letras minúsculas, exceto no caso de abreviaturas
%
\keyword{imagens omnidirecionais}
\keyword{estimativa de profundidade}

%\settowidth{\seclen}{1.10~}

%
% inicio do documento
%
\begin{document}

% folha de rosto
% às vezes é necessário redefinir algum comando logo antes de produzir
% a folha de rosto:
% \renewcommand{\coordname}{Coordenadora do Curso}
\maketitle

% dedicatoria
% \clearpage
% \begin{flushright}
%     \mbox{}\vfill
%     {\sffamily\itshape
%       ``If I have seen farther than others,\\
%       it is because I stood on the shoulders of giants.''\\}
%     --- \textsc{Sir~Isaac Newton}
% \end{flushright}

% agradecimentos
%\chapter*{Agradecimentos}
%Agradecimentos...



% resumo na língua do documento
\begin{abstract}
    Abstract em português
\end{abstract}

% resumo na outra língua
% como parametros devem ser passados o titulo e as palavras-chave
% na outra língua, separadas por vírgulas
\begin{englishabstract}{Depth estimation on a single omnidirectional image}{Keywords, in, English}
    Abstract in english
\end{englishabstract}

% lista de figuras
\listoffigures

% lista de tabelas
\listoftables

% lista de abreviaturas e siglas
% o parametro deve ser a abreviatura mais longa
\begin{listofabbrv}{ABREV}
    \item[ABREV] Abreviatura
    \item[SIG] Sigla
\end{listofabbrv}

% idem para a lista de símbolos
\begin{listofsymbols}{$\alpha\beta\pi\omega$}
    \item[$\sum{\frac{a}{b}}$] Somatório do produtório
    \item[$\alpha\beta\pi\omega$] Fator de inconstância do resultado
\end{listofsymbols}

% sumario
\tableofcontents

% aqui comeca o texto propriamente dito

% introducao
\chapter{Introdução}

\section{Motivação}

A estimativa de profundidade é um componente crucial para a compreensão da geometria tridimensional de uma cena. O método tradicional para estimativa de profundidade é o \textit{stereo matching}, que utiliza informação de correspondência de um par de imagens da mesma cena para estimar a profundidade de cada ponto.  Há diversos métodos desenvolvidos para essa tarefa em imagens planares \todo{citação?}, porém é uma área relativamente pouco explorada quando se tratam de imagens omnidirecionais. Imagens omnidirecionais possuem um campo de visão de 360\degree, representam toda a esfera ao redor do ponto onde foram obtidas.

\section{Objetivos}

\section{Estrutura}

\chapter{Conceitos}

\section{Imagens em perspectivas vs. omnidirecionais}

As áreas de visão computacional e processamento de imagens comumente trabalham com imagens planares, onde a luz de uma cena em três dimensões é projetada para um plano de duas dimensões através da projeção perspectiva. Imagens planares são capturadas através de câmeras que tem um funcionamento parecido com o olho humano, logo possuem um campo de visão limitado em $\phi$ graus. Normalmente, esse ângulo se estende até 180\degree, e quanto maior o campo de visão mais distorcidas são as imagens. 
A projeção perspectiva projeta pontos do espaço \textit{3D} para um plano \textit{2D}, no caso de imagens digitais, em pontos discretos. \todo{Referenciar variáveis da figura para explicar projeção perspectiva mais formalmente}

\begin{figure}
    \caption{Projeção perspectiva}
    \begin{center}
    \resizebox {15em} {!} {
	\begin{tikzpicture}
  [
    tdplot_main_coords,
    >=Stealth,
    my dashed/.style={dashed, thick, ->, shorten >=-15pt, shorten <=-15pt, every node/.append style={font=\footnotesize}},
    my box/.style={thin, gray!70},
    my blue/.style={blue, line cap=round, -{Triangle[width=3*#1]}, line width=#1, shorten >=#1*1.75pt, every node/.append style={fill, circle, inner sep=0pt, minimum size=#1*3.5pt, anchor=center, outer sep=0pt}},
    my label/.append style={midway, font=\scriptsize},
    my vectors/.style={green!50!black, {Stealth[scale=.75]}-{Stealth[scale=.75]}},
    my red/.style={thick, red, line cap=round},
    my grey/.style={gray!70},
    description/.style={draw=gray!70, thick, line cap=round, every node/.style={align=center, font=\scriptsize\sffamily, anchor=north}},
  ]
%   \draw [help lines] (-2,0,0) -- (2,0,0) node[anchor=north west]{$x$} (0,0,0) -- (0,7,0) node[anchor=north east]{$y$} (0,0,0) -- (0,0,2) node[anchor=north]{$z$} (-2,7,0) -- (2,7,0);
  \draw [my grey] (0,4,0) -- (0,7,0) (-2,7,0) -- (2,7,0);
  \coordinate (o) at (0,0,0);
  \path [draw=gray!70, text=gray, fill=gray!20, opacity=0.8, text opacity=1] (-1.5,4,1.75) coordinate (a) -- ++(0,0,-3.5) coordinate (b) -- ++(3,0,0) coordinate (c) -- ++(0,0,3.5) coordinate (d) -- cycle node [pos=.95, above, sloped, anchor=south west] {$z=f$} ;
%   \foreach \i in {a,b,c,d} \node [red, font=\scriptsize] at (\i) {\i};
  \draw [my grey] (-2,0,0) -- (2,0,0) (0,0,0) -- (0,4,0) (0,0,0) -- (0,0,2);
  \draw [thick, ->, every node/.style={font=\footnotesize, inner sep=0pt}] (o) node [anchor=north west] {$F_c$} (o) edge node [pos=1, anchor=north east] {$z_c$} ++(0,1,0) edge node [pos=1, anchor=north] {$y_c$} ++(0,0,1) -- ++(1,0,0) node [anchor=north west] {$x_c$};
  \draw [my box] (o) ++(0,4,-.5) coordinate (p1) -- ++(1,0,0) coordinate (p2) -- ++(0,0,-1.25) coordinate (p3);
  \foreach \i in {0,1,...,4} \draw [my box] (p1) ++(\i*.25,0,0) -- ++(0,0,-.25);
  \foreach \i in {0,1,...,5} \draw [my box] (p2) ++(0,0,-\i*.25) -- ++(-.25,0,0);
  \draw [my box] (p1) ++(0,0,-.25) -- ++(.75,0,0) -- ++(0,0,-1);
  \draw [my dashed, cyan] ($(b)!1/2!(c)$) -- ($(d)!1/2!(a)$) node [below=15pt, anchor=north] {$y$};
  \draw [my dashed, cyan] ($(b)!1/2!(a)$) -- ($(d)!1/2!(c)$) node [above right=17pt, anchor=north west] {$x$};
  \draw [my dashed, green!50!black, <->] (a) node [below=15pt, anchor=north] {$v$} -- (b) -- (c) node [above right=17pt, anchor=north west] {$u$};
  \path [green!50!black, every node/.style={font=\scriptsize, inner sep=0pt}] (p2) node [above right, anchor=south west] {$(u,v)$};
  \path (p2) ++(-.125,0,0) coordinate (q2) ++(0,0,-.125) coordinate (r2);
  \draw [my blue=1] ($(0,4,0)+($(q2)-(p1)$)$) coordinate (s2) -- (r2) node (d1) {};
  \scoped[on background layer]{\draw [my blue=1.75] ($($1.75*($(s2)-(0,4,0)$)$)+(0,7,0)$) -- ++($1.75*($(r2)-(s2)$)$) node (d2) [label={[label distance=-20pt]above:{$P=(X,Y,Z)$}}] {};}
  \draw [my vectors] (0,4,.1) -- ($(s2)+(0,0,.1)$) node [below, my label, sloped] {$\vec{u}$};
  \draw [my vectors] (-.1,4,0) -- ($(q2)-(s2)+(-.1,4,0)$) node [left, my label] {$\vec{v}$};
  \draw [my red] (o) -- (d1.center);
  \scoped[on background layer]{\draw [my red] (d1.center) -- (d2.center);}
  \path [description] (0,4,0) [out=-95, in=95] to (-.75,4,.25) node {ponto\\principal} (0,6.5,0) [out=-95, in=95] to (-.75,6.5,.25) node {eixo\\\'otico};
\end{tikzpicture}
	}
    \end{center}
    \label{fig:perspectiveProj}
    \legend{Fonte: \LaTeX Stack Exchange
    (https://tex.stackexchange.com/q/96074/)}
\end{figure}

Imagens omnidirecionais, também conhecidas como imagens esféricas, capturam a luz de todas as direções ao redor do ponto onde são capturadas, possuindo um campo de visão de 360\degree, logo as distorções que seriam introduzidas com a projeção perspectiva são muito mais significativas. Para trabalhar com imagens esféricas são utilizadas diferentes projeções para mapear a \crj{superfície da esfera de raio unitário, com duas dimensões, para um domínio planar.}. Há as projeções cilíndricas e azimutais, entre outras \todo{citar panotools wiki ou encontrar melhor}, e ao longo do trabalho será utilizada a projeção equiretangular somente. Nesta projeção, como descrito na figura \ref{fig:sphericalIm}, os 360\degree  do campo de visão horizontal são projetados em $W$ pixels e os 180\degree de campo de visão vertical em $H$ pixels.\todo{Formalizar melhor}. \crj{`Seria bom colocar as equações de mapemanento.}

Imagens omnidirecionais podem ser obtidas de múltiplas maneiras, entre elas:
\begin{itemize}
\item Uma única câmera e lentes ou espelhos especias (cônicos, esféricos, hiperbólicos, etc.). As lentes ou espelhos permitem que a câmera possa capturar toda a cena, ou pelo menos aumentem seu campo de visão. Exemplos são descritos em \citet{omniSingleCam1997} e \citet{omniSingleCam1998}.
\item Múltiplas imagens em perspectiva, obtidas por múltiplas câmeras ou por uma câmera em movimento, e colando as imagens em uma única imagem panorâmica. Conhecido também como mosaico, há exemplos em \citet{omniMultiCam1994}, \citet{omniMultiCam1996} e \citet{omniMultiCam1997}.
\item Obter diretamente um par estéreo de imagens omnidirecionais, mesmo de uma única câmera, como descrito em \citet{omniStereoCam1999}, \citep{omniStereoCam2013} e \citet{omniStereoCam2016}. \crj{Lorenzo, esse bullet não seria pra 3d?}
\end{itemize}


\begin{figure}
    \caption{Imagem omnidirecional em projeção equiretangular}
    \begin{center}
        \includegraphics[width=20em]{equiretangular.png}
    \end{center}
    \label{fig:sphericalIm}
    \legend{Fonte: Jianxiong Xiao, 3D Geometry for panorama}
\end{figure}

\section{Estimativa de profundidade}

A estimativa de profundidade consiste em estimar
a distância entre objetos contidos em uma ou mais imagens e uma câmera de referência. Há múltiplas aplicações em robótica, como navegação e reconhecimento de objetos, além de ser um componente essencial para a reconstrução 3D. No caso de imagens esféricas, a reconstrução 3D de uma imagem 360\degree tem aplicações diretas em realidade virtual e realidade aumentada.

O método clássico para estimativa de profundidade é o \textit{stereo matching} \crj{Podes trocar stereo matching por casamento estéreo}, onde a profundidade é estimada utilizando a correspondência entre pixels das duas imagens. Para tanto, é preciso utilizar câmeras cujos parâmetros sejam conhecidos. Recentemente, a aplicação de redes neurais convolucionais mostrou resultados impressionantes na estimativa de profundidade a partir de uma única imagem. No entanto, a utilização destes algoritmos com imagens 360\degree não produz a mesma qualidade de resultados, pois o treinamento destes é feito utilizando imagens perspectivas. 

O método proposto neste trabalho pretende estender as técnicas de estimativa de profundidade a partir de uma única imagem em perspectiva para o domínio das imagens esféricas. Para tanto, são feitas projeções planares de seções da imagem esférica, e é estimada a profundidade de cada seção. As profundidades calculadas são projetadas de volta para o domínio esférico, porém as profundidades estimadas para cada seção não são coerentes entre si.\crj{Podes colocar uma imagem aqui para ilustrar.}
Para gerar uma mapa de profundidades suave são escolhidas seções com regiões sobrepostas, então é feita uma minimização da diferença das profundidades nas regiões sobrepostas através da ponderação da profundidade de cada seção na imagem final.


\chapter{Trabalhos anteriores}
\crj{Acho que podes deixar a estrutura atual, mas deixar claro que o foco é no estéreo com uma única imagem.}

\section{Estimativa de profundidade}

\subsection{Múltiplas imagens}
Stereo: \citet{stereoSurvey2001},  \citet{stereoSurvey2016}

Multiview: \citet{multiViewStereo2015}

Structure from motion: \citet{structMotion2016}
Focus/defocus: \citet{defocus1987}, atual: \citet{defocus2015}
Photometric: \citet{photometricStereo1989}, \citet{photometricStereo2012}

\subsection{Única imagem}

Supervised learning: \citet{Saxena2005}, \citet{Saxena2008}

Semantic labels: \citet{Liu2010}

Convolutional Neural networks: 
\citet{Fayao2015}, \citet{Fayao2016}, \citet{Eigen2014}, \citet{Eigen2015}, \citet{Kuznietsov2017}, \citet{Li2017}

Unsupervised: \citet{Godard2016}, \citet{Zhou2017}


\section{Imagens omnidirecionais}

\subsection{\textit{Stereo} omnidirecional}
Multiview stereo omnidirecional: \citet{Li2001}

Hardware especializado para stereo omnidirecional (robotics): \citet{gluckman1998}, \citet{Koyasu2001} (+atual): \citet{Lin2014}

Single image depth with structured light: \citet{Orghidan2005}

\subsection{Outros}
Projeção da esfera para plano: \citet{sun360}

Extensão de técnicas planares para 360: \citet{flat2sphere}

3D reconstruction from spherical images: \citet{panoContext}

\chapter{O método proposto}

\section{Visão geral do método}

O método proposto neste trabalho pretende estender qualquer técnica de estimativa de profundidade de imagens planares para ser utilizada com imagens esféricas. O método consiste de múltiplas etapas sequenciais, que chamaremos de nosso \textit{pipeline}. A imagem original esférica, na projeção equiretangular passa pelas etapas de processamento descritas na Figura~\ref{fig:pipeline}.

\begin{figure}
    \caption{Etapas do \textit{pipeline}}
    \begin{center}
        \begin{picture}(450,100)
            \put(0,20){\line(0,1){50}}
            \put(0,20){\line(1,0){90}}
            \put(90,70){\line(0,-1){50}}
            \put(90,70){\line(-1,0){90}}
            \put(90,45){\vector(1,0){10}}
            \put(10,48){Seccionamento}
            \put(20,38){e projeção}
            \put(100,20){\line(0,1){50}}
            \put(100,20){\line(1,0){90}}
            \put(190,70){\line(0,-1){50}}
            \put(190,70){\line(-1,0){90}}
            \put(190,45){\vector(1,0){10}}
            \put(120,48){Estimativa}
            \put(108,38){de profundidade}
            \put(200,20){\line(0,1){50}}
            \put(200,20){\line(1,0){70}}
            \put(270,70){\line(0,-1){50}}
            \put(270,70){\line(-1,0){70}}
            \put(270,45){\vector(1,0){10}}
            \put(207,48){Reprojeção}
            \put(208,38){para esfera}
            \put(280,20){\line(0,1){50}}
            \put(280,20){\line(1,0){70}}
            \put(350,70){\line(0,-1){50}}
            \put(350,70){\line(-1,0){70}}
            \put(350,45){\vector(1,0){10}}
            \put(287,42.5){Ponderação}
            \put(360,20){\line(0,1){50}}
            \put(360,20){\line(1,0){80}}
            \put(440,70){\line(0,-1){50}}
            \put(440,70){\line(-1,0){80}}
            \put(366,42.5){Reconstrução}
        \end{picture}
    \end{center}
    \label{fig:pipeline}
    \legend{Fonte: O Autor}
\end{figure}

Inicialmente, a imagem esférica na projeção equiretangular é dividida em várias seções com sobreposição, e estas são projetadas para imagens planares. A estimativa de profundidade é realizada nestas projeções planares, e qualquer técnicas de estimativa de profundidade a partir de uma única imagem pode ser utilizada para isso.  As profundidades estimadas são projetadas de volta para o domínio esférico, formando um conjunto de imagens equiretangulares onde temos apenas uma parte da profundidade da esfera em cada uma. As técnicas utilizadas para estimativa de profundidade, no entanto, não impõem coerência entre as profundidades estimadas em cada seção da esfera, requerendo uma fusão das profundidades geradas em cada seção a fim de aprimorar a coerência global entre as estimativas de cada seção. Por fim, o método minimiza a diferença entre as profundidades de cada seção nas regiões de sobreposição. Como ainda há casos onde a diferença pode ser consideravelmente diferente de zero, é feito um \textit{blending} entre as profundidades estimadas para cada seção a fim de gerar um mapa de profundidades suave. Cada etapa será descrita em detalhes nas seções a seguir.

\section{Seccionamento e projeção para o plano}

Esfera seccionada em $N$ planos, com sobreposição porque precisamos de informação redundante para gerar a coerência global. Cada plano com FOV de 90\degree, e 45\degree de diferença entre as seções, logo há 45\degree de overlap.

A projeção é calculada utilizando a distância focal conhecida $2\pi W$ (Figura \ref{fig:sphericalIm})

Pega tamanho de imagem destino e ângulos de projeção phi, theta

calcula para cada pixel os ângulos phii thetai do pixel

divide os ângulos pela distancia focal 2piw ou pi*H para encontrar os pixels da imagem esférica que correspondem.

interpola a imagem esférica porque os pixels não são inteiros.



\section{Estimativa de profundidade}

O método é independente da técnica de estimativa de profundidade utilizada, só é necessário que seja uma técnica de estimativa a partir de uma única imagem. Os resultados com redes neurais são impressionantes, escolhemos a rede do Fayao por ter código disponível. Utilizamos as redes como um \textit{blackbox}.

Código usado foi de \citet{Fayao2016}.

\section{Projetando a seção de volta para a esfera}

Pega todos os ângulos que tem na imagem planar (precisa do FOV)

Converte os ângulos pros index dos pixels.

faz o validmap com os pixels da esfera que pertencem à seção.

interpola a imagem planar pra formar a seção esférica

\section{Ponderação dos mapas de profundidade de cada seção}

Temos várias seções esféricas com overlap, sabemos que nos overlaps queremos que os valores sejam iguais. Para cada overlap, geramos pesos linha a linha para ponderar os valores de profundidade. (Equação do sistema). A solução do sistema se dá por SVD (citar?), mas podemos variar as ponderações das equações entre pesos ou entre profundidades.

Foi feita uma segunda formulação com um peso por pixel, mas a complexidade polinomial da solução por SVD tornou a solução imprática para imagens de tamanho real. É possível fazer um \textit{downscaling} e ter um peso a cada x pixels, mas ainda assim fica com muitos artefatos...\crj{Acho que podes deixar isso como future work.}

Cada seção da esfera tem duas regiões de overlap, que geram pesos diferentes em cada parte da imagem. É feita uma interpolação destes pesos à medida que se encontra a metade da seção esférica. Cada seção gera tem então mapas de disparidades mais coerentes entre si.

\section{Reconstrução do mapa de profundidade completo}

Ainda há diferença entre os mapas, então região de overlap tem que ser blended. Fizemos um alpha blending simples baseado na proximidade do pixel de cada projeção, para gerar um mapa de profundidade suave. Gera alguns artefatos na borda das regiões de overlap, onde não tem blending.

\chapter{Resultados}

\section{Visualização}

\todo{Colocar alguns mapas gerados, talvez vistas da point cloud?}

\section{Dataset}
\todo{Vistas sintéticas com o ground truth, perguntar p/ Thiago os detalhes da cena}
\todo{Pegar um dataset do SUN360 para visualizar somente}

\section{Métricas}

Métricas do Eigen (\citep{Eigen2014}), seção 3.2, 

\begin{table}[h]
    \caption{Tabela com métricas dos resultados}
    \centering
        \begin{tabular}{c|c|c|c|c|c}
          \hline
          \textit{Imagem} & \textit{Métrica 1 seções} & \textit{Métrica 1 CNN direto} &  & \textit{Métrica 2 seções} & \textit{Métrica 2 CNN direto} \\
          \hline
          \hline
          classroom000  &  0,2  &  0,2  &  0,2  &  0,2  \\
          classroom001  &  0,2  &  0,2  &  0,2  &  0,2  \\
          \hline
        \end{tabular}
    \legend{Fonte: O Autor}
    \label{tbl:results1}
\end{table}


\chapter{Conclusões}

\bibliographystyle{abntex2-alf}
\bibliography{biblio}

\end{document}


% Exxeplos de uso de figuras

Esta seção faz referência às Figuras~\ref{fig:estrutura},~\ref{fig:ex1} e~\ref{fig:ex2}, a título de exemplo. A primeira figura apresenta a estrutura de uma figura. A \emph{descrição} deve aparecer \textbf{acima} da figura. Abaixo da figura, deve ser indicado a origem da imagem, mesmo se essa for apenas os autores do texto.

% \begin{figure}[h]
%     \caption{Descrição da Figura deve ir no topo}
%     \begin{center}
%         % Aqui vai um includegraphics , um picture environment ou qualquer
%         % outro comando necessário para incorporar o formato de imagem
%         % utilizado.
%         \begin{picture}(100,100)
%             \put(0,0){\line(0,1){100}}
%             \put(0,0){\line(1,0){100}}
%             \put(100,100){\line(0,-1){100}}
%             \put(100,100){\line(-1,0){100}}
%             \put(10,50){Uma Imagem}
%         \end{picture}
%     \end{center}
%     \label{fig:estrutura}
%     \legend{Fonte: Os Autores}
% \end{figure}

% \begin{figure}
%     \caption{Exemplo de figura importada de um arquivo e também exemplo de caption muito grande que ocupa mais de uma linha na Lista~de~Figuras}
%     \begin{center}
%         \includegraphics[width=10em]{fig}
%     \end{center}
%     \legend{Fonte: Os Autores}
%     \label{fig:ex1}
% \end{figure}

% % o `[h]' abaixo é um parâmetro opcional que sugere que o LaTeX coloque a
% % figura exatamente neste ponto do texto. Somente preocupe-se com esse tipo
% % de formatação quando o texto estiver completamente pronto (uma frase a mais
% % pode fazer o LaTeX mudar completamente de idéia sobre onde colocar as
% % figuras e tabelas)
% % \begin{figure}[h]
% \begin{figure}
%     \caption{Exemplo de figura desenhada com o environment \texttt{picture}.}
%     \begin{center}
%         \setlength{\unitlength}{.1em}
%         \begin{picture}(100,100)
%             \put(20,20){\circle{20}}
%             \put(20,20){\small\makebox(0,0){a}}
%             \put(80,80){\circle{20}}
%             \put(80,80){\small\makebox(0,0){b}}
%             \put(28,28){\vector(1,1){44}}
%         \end{picture}
%     \end{center}
%     \legend{Fonte: Os Autores}
%     \label{fig:ex2}
% \end{figure}